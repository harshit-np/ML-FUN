{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'joblib' from 'sklearn.externals' (C:\\Users\\harsh\\anaconda3\\lib\\site-packages\\sklearn\\externals\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-8675649281bd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;31m# ML Libraries :\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexternals\u001b[0m                \u001b[1;32mimport\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m            \u001b[1;32mimport\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m            \u001b[1;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNormalizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRobustScaler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMaxAbsScaler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'joblib' from 'sklearn.externals' (C:\\Users\\harsh\\anaconda3\\lib\\site-packages\\sklearn\\externals\\__init__.py)"
     ]
    }
   ],
   "source": [
    "# Warning Libraries :\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Scientific and Data Manipulation Libraries :\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import gc\n",
    "import os\n",
    "\n",
    "# NLP Libraries :\n",
    "# import nltk\n",
    "# from nltk.tokenize                    import word_tokenize\n",
    "# from nltk                             import pos_tag\n",
    "# from nltk.corpus                      import wordnet as wn\n",
    "# from nltk.corpus                      import stopwords\n",
    "# from nltk.stem                        import WordNetLemmatizer\n",
    "# from collections                      import defaultdict\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('averaged_perceptron_tagger') \n",
    "# nltk.download('stopwords')\n",
    "\n",
    "# ML Libraries :\n",
    "from sklearn.externals                import joblib\n",
    "from sklearn.preprocessing            import LabelEncoder, OneHotEncoder \n",
    "from sklearn.preprocessing            import StandardScaler, MinMaxScaler, Normalizer, RobustScaler, MaxAbsScaler\n",
    "from sklearn.feature_extraction.text  import TfidfVectorizer\n",
    "from sklearn.model_selection          import KFold, StratifiedKFold, train_test_split, cross_val_score\n",
    "from sklearn.linear_model             import MultiTaskElasticNet, ElasticNet, Lasso, RidgeClassifier, SGDClassifier, PassiveAggressiveClassifier, LogisticRegression, LinearRegression\n",
    "from sklearn.neighbors                import KNeighborsClassifier\n",
    "from sklearn.svm                      import SVC\n",
    "from sklearn.tree                     import DecisionTreeClassifier\n",
    "from sklearn.ensemble                 import VotingClassifier, RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes              import MultinomialNB, GaussianNB, ComplementNB\n",
    "from sklearn.discriminant_analysis    import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics                  import f1_score, accuracy_score, precision_score , recall_score, roc_auc_score, confusion_matrix, classification_report\n",
    "from sklearn.decomposition            import IncrementalPCA\n",
    "from statsmodels.stats.proportion     import proportion_confint\n",
    "\n",
    "# Boosting Algorithms :\n",
    "from xgboost                          import XGBClassifier\n",
    "from catboost                         import CatBoostClassifier\n",
    "from lightgbm                         import LGBMClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing value function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-b7c676736c74>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mpercent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mascending\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtotal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpercent\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Total'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Percent'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mmissing_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df_train' is not defined"
     ]
    }
   ],
   "source": [
    "##missing value function which return an dataframe with total null values and percentage\n",
    "def missing_data(data):\n",
    "    total = data.isnull().sum().sort_values(ascending = False)\n",
    "    percent = (data.isnull().sum()/data.isnull().count()*100).sort_values(ascending = False)\n",
    "    return pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "missing_data(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BASIC EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_unique(data):\n",
    "    for column in data.columns :\n",
    "        \n",
    "        print(\"No of Unique Values in \"+column+\" Column are : \"+str(data[column].nunique()))\n",
    "        print(\"Actual Unique Values in \"+column+\" Column are : \"+str(data[column].sort_values(ascending=True,na_position='last').unique() ))\n",
    "        print(\"\")\n",
    "display_unique(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Method 4 : Removes Data Duplicates while Retaining the First one - Similar to SQL DISTINCT :\n",
    "def remove_duplicate(data):\n",
    "    \n",
    "    print(\"BEFORE REMOVING DUPLICATES - No. of Rows = \",data.shape[0])\n",
    "    data.drop_duplicates(keep=\"first\", inplace=True) \n",
    "    print(\"AFTER REMOVING DUPLICATES  - No. of Rows = \",data.shape[0])\n",
    "    return \"Checked Duplicates\"\n",
    "# Remove Duplicates from \"train\" data :\n",
    "remove_duplicate(train)\n",
    "# No Duplicates at all !!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PLOT DISTRIBUTION FOR EACH FEATURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset_raw' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-525a87822dd4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m             \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxticks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrotation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mplot_distribution\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_raw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhspace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.45\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwspace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'dataset_raw' is not defined"
     ]
    }
   ],
   "source": [
    "# Let’s plot the distribution of each feature\n",
    "def plot_distribution(dataset, cols=5, width=20, height=15, hspace=0.2, wspace=0.5):\n",
    "    plt.style.use('seaborn-whitegrid')\n",
    "    fig = plt.figure(figsize=(width,height))\n",
    "    fig.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=wspace, hspace=hspace)\n",
    "    rows = math.ceil(float(dataset.shape[1]) / cols)\n",
    "    for i, column in enumerate(dataset.columns):\n",
    "        ax = fig.add_subplot(rows, cols, i + 1)\n",
    "        ax.set_title(column)\n",
    "        if dataset.dtypes[column] == np.object:\n",
    "            g = sns.countplot(y=column, data=dataset)\n",
    "            substrings = [s.get_text()[:18] for s in g.get_yticklabels()]\n",
    "            g.set(yticklabels=substrings)\n",
    "            plt.xticks(rotation=25)\n",
    "        else:\n",
    "            g = sns.distplot(dataset[column])\n",
    "            plt.xticks(rotation=25)\n",
    "    \n",
    "plot_distribution(dataset_raw, cols=3, width=20, height=20, hspace=0.45, wspace=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset_con' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-cd25b4e89322>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[0mg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myticklabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubstrings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mplot_bivariate_bar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_con\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'predclass'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhspace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwspace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'dataset_con' is not defined"
     ]
    }
   ],
   "source": [
    "# BIVARIATE ANALYSIS\n",
    "def plot_bivariate_bar(dataset, hue, cols=5, width=20, height=15, hspace=0.2, wspace=0.5):\n",
    "    dataset = dataset.select_dtypes(include=[np.object])\n",
    "    plt.style.use('seaborn-whitegrid')\n",
    "    fig = plt.figure(figsize=(width,height))\n",
    "    fig.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=wspace, hspace=hspace)\n",
    "    rows = math.ceil(float(dataset.shape[1]) / cols)\n",
    "    for i, column in enumerate(dataset.columns):\n",
    "        ax = fig.add_subplot(rows, cols, i + 1)\n",
    "        ax.set_title(column)\n",
    "        if dataset.dtypes[column] == np.object:\n",
    "            g = sns.countplot(y=column, hue=hue, data=dataset)\n",
    "            substrings = [s.get_text()[:10] for s in g.get_yticklabels()]\n",
    "            g.set(yticklabels=substrings)\n",
    "            \n",
    "plot_bivariate_bar(dataset_con, hue='predclass', cols=3, width=20, height=12, hspace=0.4, wspace=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-9bc9f6ce77a3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# BOX PLOT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstyle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'seaborn-whitegrid'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFacetGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_con\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'marital-status'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maspect\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m.7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mboxplot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'predclass'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'education-num'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# BOX PLOT\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "g = sns.FacetGrid(dataset_con, col='marital-status', size=4, aspect=.7)\n",
    "g = g.map(sns.boxplot, 'predclass', 'education-num')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    knn_model,\n",
    "    nb_model,\n",
    "    xgb_model,\n",
    "    XGB_model,\n",
    "    rf_model,\n",
    "      \n",
    "]\n",
    "\n",
    "for model in models:\n",
    "    names = model.__class__.__name__\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"-\"*28)\n",
    "    print(names + \":\" )\n",
    "    print(\"Accuracy: {:.4%}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cofusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred_XGB_model)\n",
    "plt.rcParams['figure.figsize'] = (5, 5)\n",
    "sns.heatmap(cm, annot = True, annot_kws = {'size':15}, cmap = 'PuBu',fmt=\".1f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Random Forest to gain an insight on Feature Importance\n",
    "def feature_imp_rfc(X,Y)\n",
    "    clf = RandomForestClassifier()\n",
    "    clf.fit(X,Y)\n",
    "    \n",
    "    plt.style.use('seaborn-whitegrid')\n",
    "    importance = clf.feature_importances_\n",
    "    importance = pd.DataFrame(importance, index=X.columns, columns=[\"Importance\"])\n",
    "    importance.sort_values(by='Importance', ascending=True).plot(kind='barh', figsize=(20,len(importance)/2));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_imp(model,df):\n",
    "    Importance = pd.DataFrame({\"Importance\": model.feature_importances_*100},\n",
    "                         index = df.columns)\n",
    "    Importance.sort_values(by = \"Importance\", \n",
    "                       axis = 0, \n",
    "                       ascending = True).plot(kind =\"barh\", color = \"r\")\n",
    "\n",
    "    plt.xlabel(\"Variable Significance Levels\")\n",
    "    return Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "clf.fit(X_train, Y_train)\n",
    "\n",
    "### FEATURE SCORES \n",
    "feature_scores = pd.Series(clf.feature_importances_, index=X_train.columns).sort_values(ascending=False)\n",
    "feature_scores\n",
    "\n",
    "### PLOT TO VISUALIZE\n",
    "sns.barplot(x=feature_scores, y=feature_scores.index)\n",
    "# Add labels to the graph\n",
    "plt.xlabel('Feature Importance Score')\n",
    "plt.ylabel('Features')\n",
    "# Add title to the graph\n",
    "plt.title(\"Visualizing Important Features\")\n",
    "# Visualize the graph\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation(df):\n",
    "    corr = df.corr()\n",
    "    plt.figure(figsize = (10,5))\n",
    "    sns.heatmap(corr, annot = True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Duplicates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def duplicate_drop(data):\n",
    "    data.drop_duplicates(keep='first',inplace=True)\n",
    "    print(\"Removed Duplicates\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oic\\eulernmdfj\n"
     ]
    }
   ],
   "source": [
    "print('oic'eulernmdfj\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For date and time realted preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_encoding( encoding_strategy , encoding_data , encoding_columns ):\n",
    "    \n",
    "    if encoding_strategy == \"LabelEncoding\":\n",
    "        print(\"IF LabelEncoding\")\n",
    "        Encoder = LabelEncoder()\n",
    "        for column in encoding_columns :\n",
    "            print(\"column\",column )\n",
    "            encoding_data[ column ] = Encoder.fit_transform(tuple(encoding_data[ column ]))\n",
    "        \n",
    "    elif encoding_strategy == \"OneHotEncoding\":\n",
    "        print(\"ELIF OneHotEncoding\")\n",
    "        encoding_data = pd.get_dummies(encoding_data)\n",
    "        \n",
    "    elif encoding_strategy == \"TargetEncoding\":\n",
    "        print(\"ELIF TargetEncoding\")\n",
    "        ## Code Coming soon\n",
    "        print(\"TargetEncoding\")\n",
    "\n",
    "    else :\n",
    "        print(\"ELSE OneHotEncoding\")\n",
    "        encoding_data = pd.get_dummies(encoding_data)\n",
    "        \n",
    "    dtypes_list =['float64','float32','int64','int32']\n",
    "    encoding_data.astype( dtypes_list[0] ).dtypes\n",
    "    \n",
    "    return encoding_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KFOLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf =KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "cnt = 1\n",
    "# split()  method generate indices to split data into training and test set.\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')\n",
    "    cnt += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature Reduction / Selection\n",
    "Once we have our features ready to use, we might find that the number of features available is too large to be run in a reasonable timeframe by our machine learning algorithms. There's a number of options available to us for feature reduction and feature selection.\n",
    "\n",
    "##### Dimensionality Reduction:\n",
    "`Principal Component Analysis (PCA)`: Principal component analysis (PCA) is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components. This transformation is defined in such a way that the first principal component has the largest possible variance (that is, accounts for as much of the variability in the data as possible), and each succeeding component in turn has the highest variance possible under the constraint that it is orthogonal to the preceding components.\n",
    "\n",
    "##### Singular Value Decomposition (SVD): \n",
    "SVD is a factorization of a real or complex matrix. It is the generalization of the eigendecomposition of a positive semidefinite normal matrix (for example, a symmetric matrix with positive eigenvalues) to any m×n matrix via an extension of the polar decomposition. It has many useful applications in signal processing and statistics.\n",
    "\n",
    "##### Feature Importance/Relevance:\n",
    "`Filter Methods`: Filter type methods select features based only on general metrics like the correlation with the variable to predict. Filter methods suppress the least interesting variables. The other variables will be part of a classification or a regression model used to classify or to predict data. These methods are particularly effective in computation time and robust to overfitting.\n",
    "`Wrapper Methods`: Wrapper methods evaluate subsets of variables which allows, unlike filter approaches, to detect the possible interactions between variables. The two main disadvantages of these methods are : The increasing overfitting risk when the number of observations is insufficient. AND. The significant computation time when the number of variables is large.\n",
    "`Embedded Methods`: Embedded methods try to combine the advantages of both previous methods. A learning algorithm takes advantage of its own variable selection process and performs feature selection and classification simultaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FEATURE SCALING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_scaling( scaling_strategy , scaling_data , scaling_columns ):\n",
    "    \n",
    "    if    scaling_strategy ==\"RobustScaler\" :\n",
    "        scaling_data[scaling_columns] = RobustScaler().fit_transform(scaling_data[scaling_columns])\n",
    "        \n",
    "    elif  scaling_strategy ==\"StandardScaler\" :\n",
    "        scaling_data[scaling_columns] = StandardScaler().fit_transform(scaling_data[scaling_columns])\n",
    "        \n",
    "    elif  scaling_strategy ==\"MinMaxScaler\" :\n",
    "        scaling_data[scaling_columns] = MinMaxScaler().fit_transform(scaling_data[scaling_columns])\n",
    "        \n",
    "    elif  scaling_strategy ==\"MaxAbsScaler\" :\n",
    "        scaling_data[scaling_columns] = MaxAbsScaler().fit_transform(scaling_data[scaling_columns])\n",
    "        \n",
    "    else :  # If any other scaling send by mistake still perform Robust Scalar\n",
    "        scaling_data[scaling_columns] = RobustScaler().fit_transform(scaling_data[scaling_columns])\n",
    "    \n",
    "    return scaling_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA\n",
    "`Principal component analysis (PCA)` is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components. This transformation is defined in such a way that the first principal component has the largest possible variance (that is, accounts for as much of the variability in the data as possible), and each succeeding component in turn has the highest variance possible under the constraint that it is orthogonal to the preceding components.\n",
    "\n",
    "We can use PCA to reduce the number of features to use in our ML algorithms, and graphing the variance gives us an idea of how many features we really need to represent our dataset fully.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating PCA for both datasets, and graphing the Variance for each feature, per dataset\n",
    "std_scale = preprocessing.StandardScaler().fit(dataset_bin_enc.drop('predclass', axis=1))\n",
    "X = std_scale.transform(dataset_bin_enc.drop('predclass', axis=1))\n",
    "pca1 = PCA(n_components=len(dataset_bin_enc.columns)-1)\n",
    "fit1 = pca1.fit(X)\n",
    "\n",
    "std_scale = preprocessing.StandardScaler().fit(dataset_con_enc.drop('predclass', axis=1))\n",
    "X = std_scale.transform(dataset_con_enc.drop('predclass', axis=1))\n",
    "pca2 = PCA(n_components=len(dataset_con_enc.columns)-2)\n",
    "fit2 = pca2.fit(X)\n",
    "\n",
    "# Graphing the variance per feature\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "plt.figure(figsize=(25,7)) \n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.xlabel('PCA Feature')\n",
    "plt.ylabel('Variance')\n",
    "plt.title('PCA for Discretised Dataset')\n",
    "plt.bar(range(0, fit1.explained_variance_ratio_.size), fit1.explained_variance_ratio_);\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.xlabel('PCA Feature')\n",
    "plt.ylabel('Variance')\n",
    "plt.title('PCA for Continuous Dataset')\n",
    "plt.bar(range(0, fit2.explained_variance_ratio_.size), fit2.explained_variance_ratio_);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA's components graphed in 2D and 3D\n",
    "# Apply Scaling \n",
    "std_scale = preprocessing.StandardScaler().fit(dataset_con_enc.drop('predclass', axis=1))\n",
    "X = std_scale.transform(dataset_con_enc.drop('predclass', axis=1))\n",
    "y = dataset_con_enc['predclass']\n",
    "\n",
    "# Formatting\n",
    "target_names = [0,1]\n",
    "colors = ['navy','darkorange']\n",
    "lw = 2\n",
    "alpha = 0.3\n",
    "# 2 Components PCA\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "plt.figure(2, figsize=(20, 8))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "pca = PCA(n_components=2)\n",
    "X_r = pca.fit(X).transform(X)\n",
    "for color, i, target_name in zip(colors, [0, 1], target_names):\n",
    "    plt.scatter(X_r[y == i, 0], X_r[y == i, 1], \n",
    "                color=color, \n",
    "                alpha=alpha, \n",
    "                lw=lw,\n",
    "                label=target_name)\n",
    "plt.legend(loc='best', shadow=False, scatterpoints=1)\n",
    "plt.title('First two PCA directions');\n",
    "\n",
    "# 3 Components PCA\n",
    "ax = plt.subplot(1, 2, 2, projection='3d')\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "X_reduced = pca.fit(X).transform(X)\n",
    "for color, i, target_name in zip(colors, [0, 1], target_names):\n",
    "    ax.scatter(X_reduced[y == i, 0], X_reduced[y == i, 1], X_reduced[y == i, 2], \n",
    "               color=color,\n",
    "               alpha=alpha,\n",
    "               lw=lw, \n",
    "               label=target_name)\n",
    "plt.legend(loc='best', shadow=False, scatterpoints=1)\n",
    "ax.set_title(\"First three PCA directions\")\n",
    "ax.set_xlabel(\"1st eigenvector\")\n",
    "ax.set_ylabel(\"2nd eigenvector\")\n",
    "ax.set_zlabel(\"3rd eigenvector\")\n",
    "\n",
    "# rotate the axes\n",
    "ax.view_init(30, 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursive Feature Elimination\n",
    "Feature ranking with recursive feature elimination and cross-validated selection of the best number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating RFE for non-discretised dataset, and graphing the Importance for each feature, per dataset\n",
    "selector1 = RFECV(LogisticRegression(), step=1, cv=5, n_jobs=-1)\n",
    "selector1 = selector1.fit(dataset_con_enc.drop('predclass', axis=1).values, dataset_con_enc['predclass'].values)\n",
    "print(\"Feature Ranking For Non-Discretised: %s\" % selector1.ranking_)\n",
    "print(\"Optimal number of features : %d\" % selector1.n_features_)\n",
    "# Plot number of features VS. cross-validation scores\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "plt.figure(figsize=(20,5)) \n",
    "plt.xlabel(\"Number of features selected - Non-Discretised\")\n",
    "plt.ylabel(\"Cross validation score (nb of correct classifications)\")\n",
    "plt.plot(range(1, len(selector1.grid_scores_) + 1), selector1.grid_scores_);\n",
    "\n",
    "# Feature space could be subsetted like so:\n",
    "dataset_con_enc = dataset_con_enc[dataset_con_enc.columns[np.insert(selector1.support_, 0, True)]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "train_pred = model.predict(train_X)\n",
    "for thresh  in np.arange(0,1.05,0.025):\n",
    "    train_pred_thresh = [1 if i>thresh else 0 for i in list(train_pred)]\n",
    "    roc_score = roc_auc_score(y,train_pred_thresh)\n",
    "    print(f\"thresh:{thresh},roc:{roc_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare_tree_based_models with graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_tree_based_models():\n",
    "    random_state = 2\n",
    "    classifiers = [SVC(random_state=random_state), DecisionTreeClassifier(random_state=random_state),\n",
    "                   AdaBoostClassifier(DecisionTreeClassifier(random_state=random_state), \n",
    "                                      random_state=random_state,\n",
    "                                      learning_rate=0.1), \n",
    "                   \n",
    "                   RandomForestClassifier(random_state=random_state),\n",
    "                   \n",
    "                   ExtraTreesClassifier(random_state=random_state),\n",
    "                   \n",
    "                   GradientBoostingClassifier(random_state=random_state),\n",
    "                   \n",
    "                   MLPClassifier(random_state=random_state),\n",
    "                   \n",
    "                   KNeighborsClassifier(), \n",
    "                   \n",
    "                   LogisticRegression(random_state=random_state), \n",
    "                   \n",
    "                   LinearDiscriminantAnalysis()]\n",
    "\n",
    "    cv_results = []\n",
    "    for classifier in classifiers:\n",
    "        cv_results.append(cross_val_score(classifier, X_train, y=y_train,\n",
    "                                          scoring=\"accuracy\",\n",
    "                                          cv=4, n_jobs=4))\n",
    "\n",
    "    cv_means = []\n",
    "    cv_std = []\n",
    "    for cv_result in cv_results:\n",
    "        cv_means.append(cv_result.mean())\n",
    "        cv_std.append(cv_result.std())\n",
    "\n",
    "    cv_res = pd.DataFrame(\n",
    "        {\"CrossValMeans\": cv_means, \n",
    "         \"CrossValerrors\": cv_std, \n",
    "         \"Algorithm\": [\"SVC\", \"DecisionTree\", \"AdaBoost\",\"RandomForest\", \"ExtraTrees\",\n",
    "                        \"GradientBoosting\",\"MultipleLayerPerceptron\", \"KNeighboors\",\n",
    "                        \"LogisticRegression\",\"LinearDiscriminantAnalysis\"]})\n",
    "\n",
    "    g = sns.barplot(\"CrossValMeans\", \"Algorithm\",\n",
    "                    data=cv_res, palette=\"Set3\",\n",
    "                    orient=\"h\", **{'xerr': cv_std})\n",
    "    g.set_xlabel(\"Mean Accuracy\")\n",
    "    g = g.set_title(\"Cross validation scores\")\n",
    "    plt.show()\n",
    "\n",
    "compare_tree_based_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-https://www.kaggle.com/backtracking/defcon-challenge-hackerearth/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotLearningCurve(_model,_ColList,_xtrain,_ytrain,_cv=5):\n",
    "    from sklearn.model_selection import learning_curve\n",
    "    _LC_output=learning_curve(_model, _xtrain[_ColList], _ytrain, n_jobs=-1, cv=_cv, train_sizes=np.linspace(.1, 1.0, 5), verbose=0)\n",
    "\n",
    "    _train_sizes, _train_scores, _test_scores =_LC_output[0],_LC_output[1],_LC_output[2]\n",
    "\n",
    "    _train_scores_mean,_train_scores_std=np.mean(_LC_output[1], axis=1),np.std(_LC_output[1], axis=1)\n",
    "\n",
    "    _test_scores_mean,_test_scores_std=np.mean(_LC_output[2], axis=1),np.std(_LC_output[2], axis=1)\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.title(\"\"\"Learning Curve to Understand wheather model has high bias, high variance or is ideal\\n\"\"\"+str(_train_sizes))\n",
    "    #plt.legend(loc=\"best\")\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.gca().invert_yaxis()\n",
    "    #print()\n",
    "     # plot the std deviation as a transparent range at each training set size\n",
    "    plt.fill_between(_train_sizes, _train_scores_mean - _train_scores_std, _train_scores_mean + _train_scores_std, alpha=0.1, color=\"r\")\n",
    "    plt.fill_between(_train_sizes, _test_scores_mean - _test_scores_std, _test_scores_mean + _test_scores_std, alpha=0.1, color=\"g\")\n",
    "    #print(train_scores_mean)  \n",
    "        # plot the average training and test score lines at each training set size\n",
    "    plt.plot(_train_sizes, _train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "    plt.plot(_train_sizes, _test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(model,X,Y):\n",
    "    from sklearn.metrics import roc_curve\n",
    "    pred = model.predict_proba(X)[:,1]\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(Y,pred) \n",
    "    auc = roc_auc_score(Y, pred) \n",
    "\n",
    "    #print(fpr,tpr,auc)\n",
    "    \n",
    "    plt.figure(figsize=(12,8)) \n",
    "    plt.plot(fpr,tpr,label=\"Validation AUC-ROC=\"+str(auc)) \n",
    "    x = np.linspace(0, 1, 1000)\n",
    "    plt.plot(x, x, linestyle='-')\n",
    "    plt.xlabel('False Positive Rate') \n",
    "    plt.ylabel('True Positive Rate') \n",
    "    plt.legend(loc=4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pr_curve(model,X,Y):\n",
    "    fig= plt.figure(figsize=(12,7))\n",
    "    probas = model_lr.predict_proba(X)[:,1]\n",
    "    pred = model_lr.predict(X)\n",
    "\n",
    "    #fpr, tpr, _ = roc_curve(Y,pred) \n",
    "    precision, recall, thresholds = precision_recall_curve(Y, probas)\n",
    "    f1 = f1_score(Y, pred)\n",
    "    auc_s = auc(recall, precision)\n",
    "    #f1,auc\n",
    "    print(recall[:])\n",
    "    print('Logistic: f1=%.3f auc=%.3f' % (f1, auc_s))\n",
    "    no_skill=len(Y[Y==1]) / len(Y)\n",
    "    plt.plot([0, 1], [no_skill, no_skill], marker='^',linestyle='--', label='No Skill')\n",
    "    plt.plot(recall, precision, marker='.', label='Logistic')\n",
    "    plt.ylabel('Precison',fontsize=25) \n",
    "    plt.xlabel('Recall',fontsize=25)\n",
    "    plt.legend(loc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data={'name':[1,2,3,3,3,32,2,2],\n",
    "       'age':[1,2,2,23,22,2,1,1],\n",
    "        'aukat':[12,34,53,25,35,5,542,312]}\n",
    "da=pd.DataFrame(data,columns=['name','age','aukat'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>age</th>\n",
       "      <th>aukat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>22</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>312</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   name  age  aukat\n",
       "0     1    1     12\n",
       "1     2    2     34\n",
       "2     3    2     53\n",
       "3     3   23     25\n",
       "4     3   22     35\n",
       "5    32    2      5\n",
       "6     2    1    542\n",
       "7     2    1    312"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
